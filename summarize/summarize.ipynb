{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.Venv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ModelApi:\n",
    "    def __init__(self, temperature=0.5, model=\"gpt-3.5-turbo\", max_tokens=1000):\n",
    "        \"\"\"\n",
    "        Initializes the LLM with specified parameters.\n",
    "\n",
    "        :param temperature: Controls the randomness of the output. Default is 0.5.\n",
    "        :param model: Specifies the model to use. Default is \"gpt-3.5-turbo\".\n",
    "        :param max_tokens: Maximum number of tokens to generate. Default is 1000.\n",
    "        \"\"\"\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.OpenAI = OpenAI(\n",
    "            temperature=self.temperature,\n",
    "            model=self.model,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "\n",
    "    def generate_text(self, prompt):\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt using the initialized LLM.\n",
    "\n",
    "        :param prompt: The input text prompt to generate a response for.\n",
    "        :return: Generated text response.\n",
    "        \"\"\"\n",
    "        response = self.OpenAI.invoke(input=prompt)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"from_stats_to_datascience.txt\", encoding=\"utf-8\") as f:\n",
    "    document = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/75774873/openai-api-error-this-is-a-chat-model-and-not-supported-in-the-v1-completions\n",
    "params = {\n",
    "    'temperature':0.5,\n",
    "    'model':\"gpt-3.5-turbo-instruct\", \n",
    "    'max_tokens'  : 1000\n",
    "}\n",
    "llm = ModelApi(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" doing\\n\\nI'm doing well, thank you for asking. How about you?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate_text(\"hey, how ae you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(llm):\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    ### Instruction:\n",
    "    The following text is an extract from a corpus of documents relevant to an aircraft Nacelle maintenance case.\n",
    "    Please write a concise and informative 100-word summary that captures the key points and critical information from the extract. \n",
    "    Ensure that the summary includes important details such as the main issues, actions taken, and outcomes related to the maintenance case. \n",
    "    Avoid repetition and focus on clarity and completeness. Results In english please.\n",
    "\n",
    "    {text}\n",
    "\n",
    "    ### Summary:\n",
    "    \"\"\"\n",
    "    summary_prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"text\"]\n",
    "    )\n",
    "    summarize_chain = load_summarize_chain(\n",
    "        llm, chain_type=\"map_reduce\",\n",
    "        map_prompt=summary_prompt, combine_prompt=summary_prompt\n",
    "    )\n",
    "    return summarize_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def summarize_doc(content, summarize_chain, timeout=5*60):\n",
    "\n",
    "    summary = [\"Summary failed\"]\n",
    "    if not isinstance(content, str):\n",
    "        logger.info(\"content is not string\")\n",
    "\n",
    "    else:\n",
    "        content = re.sub(r\" +\\n\", '\\n', content, 0, re.MULTILINE)\n",
    "        try:\n",
    "            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer, chunk_size=int(1024), chunk_overlap=100,\n",
    "                separators=[\"---------------page-------------\", \"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            texts = text_splitter.split_text(content)\n",
    "            docs = [Document(page_content=t) for t in texts]\n",
    "\n",
    "            summary = summarize_chain.invoke(input=docs, token_max=1800)\n",
    "        except Exception as e:\n",
    "            logger.info(content[:100])\n",
    "            logger.exception(e)\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"from_stats_to_datascience.txt\", encoding=\"utf-8\") as f:\n",
    "    document=   f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_chain  = get_chain(llm.OpenAI)\n",
    "results = summarize_doc(document, s_chain, timeout=3*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe role of statisticians has evolved with the rise of digital technologies and the increasing amount of data available. Historically focused on demographic, economic, and social data, they now use advanced techniques such as data mining and text analysis. Vectorization has greatly improved the analysis of unstructured text, while techniques like convolutional neural networks and geographic information systems are used for images and geospatial data. Data scientists have emerged to handle the growing amount of unstructured data, using their skills in statistics, programming, and visualization. However, the field continues to evolve with new techniques such as deep learning and temporal data analysis, requiring professionals to stay updated. Ethical and regulatory considerations also play a significant role in this field.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"output_text\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
